{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}\n",
    "\n",
    "Let's start by importing all necessary modules and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/HX46_FR5/repo_perso/AbXtract')\n",
    "\n",
    "\n",
    "from AbXtract import *\n",
    "from AbXtract import AntibodyDescriptorCalculator, Config, load_config\n",
    "from AbXtract.sequence import (\n",
    "    SequenceLiabilityAnalyzer,\n",
    "    BashourDescriptorCalculator,\n",
    "    PeptideDescriptorCalculator,\n",
    "    AntibodyNumbering\n",
    ")\n",
    "from AbXtract.structure import (\n",
    "    SASACalculator,\n",
    "    ChargeAnalyzer,\n",
    "    DSSPAnalyzer,\n",
    "    PropkaAnalyzer,\n",
    "    ArpeggioAnalyzer\n",
    ")\n",
    "from AbXtract.utils import (\n",
    "    read_fasta,\n",
    "    write_fasta,\n",
    "    parse_sequence,\n",
    "    validate_sequence\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default configuration\n",
    "custom_config = Config()\n",
    "\n",
    "'''\n",
    "# Test custom configuration\n",
    "custom_config = Config.from_dict({\n",
    "    'pH': 7.4,\n",
    "    'numbering_scheme': 'kabat',\n",
    "    'verbose': True,\n",
    "    'calculate_dssp': tool_status.get('dssp', False),\n",
    "    'calculate_propka': tool_status.get('propka', False),\n",
    "    'calculate_arpeggio': tool_status.get('arpeggio', False)\n",
    "})\n",
    "'''\n",
    "\n",
    "\n",
    "# Check external tool availability\n",
    "tool_status = custom_config.check_external_tools()\n",
    "print(\"üõ†Ô∏è External Tool Status:\")\n",
    "for tool, available in tool_status.items():\n",
    "    status = \"‚úÖ\" if available else \"‚ùå\"\n",
    "    print(f\"  {tool}: {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbering = AntibodyNumbering(scheme='imgt')\n",
    "peptide_calc = PeptideDescriptorCalculator()\n",
    "calc = AntibodyDescriptorCalculator(config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abxtract_path = \"/home/HX46_FR5/repo_perso/AbXtract\"\n",
    "sys.path.insert(0, abxtract_path)\n",
    "\n",
    "# Set up test data paths\n",
    "BASE_DIR = Path.cwd() \n",
    "DATA_DIR = BASE_DIR / \"data\" / \"test\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define test file paths\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input sequence and pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test antibody sequences (based on therapeutic antibodies)\n",
    "HEAVY_SEQUENCE = (\n",
    "    \"QVQLVQSGAEVKKPGASVKVSCKASGGTFSSYAISWVRQAPGQGLEWMGGIIPIFGTANYAQKFQGRVTITADESTSTAYMELSSLRSEDTAVYYCARSHYGLDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSCDKTHTCPPCPAPELLGGPSVFLFPPKPKDTLMISRTPEVTCVVVDVSHEDPEVKFNWYVDGVEVHNAKTKPREEQYASTYRVVSVLTVLHQDWLNGKEYKCKVSNKALPAPIEKTISKAKGQPREPQVYTLPPSRDELTKNQVSLTCLVKGFYPSDIAVEWESNGQPENNYKTTPPVLDSDGSFFLYSKLTVDKSRWQQGNVFSCSVMHEALHNHYTQKSLSLSPGK\"\n",
    ")\n",
    "# Light chain: Includes realistic VL domain + human kappa constant region  \n",
    "LIGHT_SEQUENCE = (\n",
    "    \"DIQMTQSPSSLSASVGDRVTITCRASHSISSYLAWYQQKPGKAPKLLIYAASSLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSTPLTFGGGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC\"\n",
    ")\n",
    "PDB_FILE = DATA_DIR / \"test.pdb\"  # User will provide this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence validity for numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_valid, heavy_msg = validate_sequence(HEAVY_SEQUENCE)\n",
    "light_valid, light_msg = validate_sequence(LIGHT_SEQUENCE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_numbered = numbering.number_sequence(HEAVY_SEQUENCE, 'H')  # Use VH portion only\n",
    "light_numbered = numbering.number_sequence(LIGHT_SEQUENCE, 'L')  # Use VH portion only\n",
    "\n",
    "annotated_H, cdrs_H = numbering.get_cdr_sequences(heavy_numbered, 'H')\n",
    "annotated_L, cdrs_L = numbering.get_cdr_sequences(light_numbered, 'L')\n",
    "\n",
    "heavy_profiles = numbering.get_peptide_profiles(HEAVY_SEQUENCE)\n",
    "light_profiles = numbering.get_peptide_profiles(LIGHT_SEQUENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Peptide descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_results = peptide_calc.calculate_all(\n",
    "    heavy_sequence=HEAVY_SEQUENCE,\n",
    "    light_sequence=LIGHT_SEQUENCE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Sequence descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_results, liabilities = calc.calculate_sequence_descriptors(\n",
    "    heavy_sequence=HEAVY_SEQUENCE,\n",
    "    light_sequence=LIGHT_SEQUENCE,\n",
    "    sequence_id=\"TestAb_Sequence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Sequence descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run structure analysis if PDB is available\n",
    "structure_results_seq, structure_results_comp, df_residues = calc.calculate_structure_descriptors(\n",
    "    pdb_file=PDB_FILE,\n",
    "    structure_id=\"TestAb_Structure\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organise outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_valid, light_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heavy_numbered, light_numbered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdrs_H, cdrs_L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotated_H, annotated_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heavy_profiles, light_profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "liabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peptide_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_results_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Residue annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating comprehensive heavy chain dataframe\n",
    "def create_comprehensive_df(annotations, hydrophobicity, chain_type='Heavy'):\n",
    "    # Start with basic annotation data\n",
    "    data = []\n",
    "    \n",
    "    for item in annotations:\n",
    "        position_tuple, amino_acid, region = item\n",
    "        position_num = position_tuple[0]\n",
    "        \n",
    "        # Get index for hydrophobicity values (0-based)\n",
    "        idx = position_num - 1\n",
    "        \n",
    "        # Create row with all information\n",
    "        row = {\n",
    "            'position': position_num,\n",
    "            'amino_acid': amino_acid,\n",
    "            'region': region,\n",
    "            'charge_sign': hydrophobicity['charge_sign'][idx] if idx < len(hydrophobicity['charge_sign']) else np.nan,\n",
    "            'hydrophobicity_hw': hydrophobicity['hydrophobicity_hw'][idx] if idx < len(hydrophobicity['hydrophobicity_hw']) else np.nan,\n",
    "            'hydrophobicity_eisenberg': hydrophobicity['hydrophobicity_eisenberg'][idx] if idx < len(hydrophobicity['hydrophobicity_eisenberg']) else np.nan,\n",
    "            'hydrophobicity_rose': hydrophobicity['hydrophobicity_rose'][idx] if idx < len(hydrophobicity['hydrophobicity_rose']) else np.nan,\n",
    "            'hydrophobicity_janin': hydrophobicity['hydrophobicity_janin'][idx] if idx < len(hydrophobicity['hydrophobicity_janin']) else np.nan,\n",
    "            'hydrophobicity_engelman': hydrophobicity['hydrophobicity_engelman'][idx] if idx < len(hydrophobicity['hydrophobicity_engelman']) else np.nan\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_liability_columns(df, chain_type, liabilities_list):\n",
    "    \"\"\"\n",
    "    Add boolean columns for each LIABILITY TYPE (not just the ones present in this chain).\n",
    "    Place these columns BEFORE the position column.\n",
    "    \"\"\"\n",
    "    chain_letter = 'H' if chain_type == 'Heavy' else 'L'\n",
    "    \n",
    "    # Define ALL possible liability types based on your liability definitions\n",
    "    all_liability_types = [\n",
    "        'Unpaired_Cys',\n",
    "        'N-linked_glycosylation',\n",
    "        'Met_oxidation',\n",
    "        'Trp_oxidation',\n",
    "        'Asn_deamidation',\n",
    "        'Asp_isomerisation',\n",
    "        'Lysine_Glycation',\n",
    "        'N-terminal_glutamate',\n",
    "        'Integrin_binding',\n",
    "        'CD11c/CD18_binding',\n",
    "        'Fragmentation',\n",
    "        'Polyreactivity'\n",
    "    ]\n",
    "    \n",
    "    # Initialize ALL liability columns as False\n",
    "    for col_name in all_liability_types:\n",
    "        df[col_name] = False\n",
    "    \n",
    "    # Now mark positions that have each liability based on the actual data\n",
    "    for liability in liabilities_list:\n",
    "        if liability['chain'] == chain_letter:\n",
    "            # Get position range\n",
    "            start_pos = liability['start_position'][0]\n",
    "            end_pos = liability['end_position'][0]\n",
    "            \n",
    "            # Create column name by simplifying the liability name\n",
    "            col_name = liability['name'].split('(')[0].strip().replace(' ', '_').replace('/', '')\n",
    "            \n",
    "            # Mark all positions in range as True\n",
    "            mask = (df['position'] >= start_pos) & (df['position'] <= end_pos)\n",
    "            if mask.any():\n",
    "                df.loc[mask, col_name] = True\n",
    "    \n",
    "    # Reorder columns: liability columns FIRST, then position, amino_acid, region, then hydrophobicity\n",
    "    liability_cols = all_liability_types\n",
    "    base_cols = ['position', 'amino_acid', 'region']\n",
    "    hydro_cols = ['charge_sign', 'hydrophobicity_hw', 'hydrophobicity_eisenberg', \n",
    "                  'hydrophobicity_rose', 'hydrophobicity_janin', 'hydrophobicity_engelman']\n",
    "    \n",
    "    # New column order: liabilities first, then base, then hydrophobicity\n",
    "    new_order = base_cols + hydro_cols + liability_cols\n",
    "    df = df[new_order]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_residue_sasa_sum_column(structures_df):\n",
    "    \"\"\"\n",
    "    Add a column with the sum of residue SASA per position to structures_results_seq dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    structures_df: DataFrame with columns including 'sidechain_sasa' containing SASA dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with added 'residue_sasa_sum' column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the new column as a list to hold dictionaries\n",
    "    residue_sasa_sum_list = []\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in structures_df.iterrows():\n",
    "        # Get the sidechain_sasa dictionary for this row\n",
    "        sidechain_sasa = row['sidechain_sasa']\n",
    "        \n",
    "        # Create a new dictionary for residue_sasa_sum\n",
    "        # The key will be the position number, value will be the SASA sum\n",
    "        residue_sasa_sum = {}\n",
    "        \n",
    "        if isinstance(sidechain_sasa, dict):\n",
    "            # Process each residue in the sidechain_sasa dictionary\n",
    "            for residue_key, sasa_value in sidechain_sasa.items():\n",
    "                # Extract position from the residue key (e.g., 'ASP_0_H' -> 0)\n",
    "                # Split by underscore and get the position (middle part)\n",
    "                parts = residue_key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        # Position in the key is 0-based, convert to 1-based for consistency\n",
    "                        position = int(parts[1]) + 1\n",
    "                        \n",
    "                        # Store the SASA sum with position as key\n",
    "                        residue_sasa_sum[position] = sasa_value\n",
    "                    except ValueError:\n",
    "                        # Skip if position is not a valid integer\n",
    "                        continue\n",
    "        \n",
    "        # Append the dictionary to the list\n",
    "        residue_sasa_sum_list.append(residue_sasa_sum)\n",
    "    \n",
    "    # Add the new column to the dataframe\n",
    "    structures_df['residue_sasa_sum'] = residue_sasa_sum_list\n",
    "    \n",
    "    return structures_df\n",
    "\n",
    "\n",
    "\n",
    "def add_structural_info_to_df(df, structures_data, chain_type='Heavy'):\n",
    "    \"\"\"\n",
    "    Add structural information from structures_results_seq to chain dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with position, amino_acid, region columns\n",
    "    structures_data: Row from structures_results_seq containing all structural data\n",
    "    chain_type: 'Heavy' or 'Light'\n",
    "    \"\"\"\n",
    "    chain_letter = 'H' if chain_type == 'Heavy' else 'L'\n",
    "    \n",
    "    # 1. Add disulfide bond information\n",
    "    df['disulfide_bond'] = False\n",
    "    disulfide_bonds = structures_data['disulfide_bonds']\n",
    "    if isinstance(disulfide_bonds, list):\n",
    "        for bond in disulfide_bonds:\n",
    "            for cys_key in ['cys1', 'cys2']:\n",
    "                if cys_key in bond:\n",
    "                    cys_info = bond[cys_key]\n",
    "                    # Check if it's for this chain\n",
    "                    if cys_info.endswith(f'_{chain_letter}'):\n",
    "                        # Extract position (e.g., 'H_21' -> 21)\n",
    "                        try:\n",
    "                            pos = int(cys_info.split('_')[1])\n",
    "                            df.loc[df['position'] == pos, 'disulfide_bond'] = True\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue\n",
    "    \n",
    "    # 2. Add SAP (Spatial Aggregation Propensity) per position\n",
    "    df['sap'] = np.nan\n",
    "    residue_sap = structures_data['residue_sap']\n",
    "    if isinstance(residue_sap, dict):\n",
    "        for key, value in residue_sap.items():\n",
    "            if key.endswith(f'_{chain_letter}'):\n",
    "                parts = key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        pos = int(parts[1]) + 1  # Convert 0-based to 1-based\n",
    "                        df.loc[df['position'] == pos, 'sap'] = value\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # 3. Add high SAP flag\n",
    "    df['high_sap'] = False\n",
    "    high_sap_residues = structures_data['high_sap_residues']\n",
    "    if isinstance(high_sap_residues, list):\n",
    "        for residue in high_sap_residues:\n",
    "            if residue.endswith(f'_{chain_letter}'):\n",
    "                parts = residue.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        pos = int(parts[1]) + 1  # Convert 0-based to 1-based\n",
    "                        df.loc[df['position'] == pos, 'high_sap'] = True\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # 4. Add sidechain SASA per position\n",
    "    df['sidechain_sasa'] = np.nan\n",
    "    sidechain_sasa = structures_data['sidechain_sasa']\n",
    "    if isinstance(sidechain_sasa, dict):\n",
    "        for key, value in sidechain_sasa.items():\n",
    "            if key.endswith(f'_{chain_letter}'):\n",
    "                parts = key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        pos = int(parts[1]) + 1  # Convert 0-based to 1-based\n",
    "                        df.loc[df['position'] == pos, 'sidechain_sasa'] = value\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # 5. Add buried residues flag\n",
    "    df['buried'] = False\n",
    "    buried_residues = structures_data['buried_residues']\n",
    "    if isinstance(buried_residues, list):\n",
    "        for residue in buried_residues:\n",
    "            if residue.endswith(f'_{chain_letter}'):\n",
    "                parts = residue.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        pos = int(parts[1]) + 1  # Convert 0-based to 1-based\n",
    "                        df.loc[df['position'] == pos, 'buried'] = True\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # 6. Add pKa values\n",
    "    df['pka'] = None\n",
    "    residue_pka = structures_data['residue_pka']\n",
    "    if isinstance(residue_pka, dict):\n",
    "        for key, value in residue_pka.items():\n",
    "            if key.endswith(f'_{chain_letter}'):\n",
    "                parts = key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        pos = int(parts[1]) + 1  # Convert 0-based to 1-based\n",
    "                        df.loc[df['position'] == pos, 'pka'] = value\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # 7. Add pKa shifts\n",
    "    df['pka_shift'] = None\n",
    "    pka_shifts = structures_data['pka_shifts']\n",
    "    if isinstance(pka_shifts, dict):\n",
    "        for key, value in pka_shifts.items():\n",
    "            if key.endswith(f'_{chain_letter}'):\n",
    "                parts = key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    try:\n",
    "                        pos = int(parts[1]) + 1  # Convert 0-based to 1-based\n",
    "                        df.loc[df['position'] == pos, 'pka_shift'] = value\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    \n",
    "    # 8. Add residue SASA sum per position\n",
    "    df['residue_sasa_sum'] = np.nan\n",
    "    residue_sasa_sum = structures_data['residue_sasa_sum']\n",
    "    if isinstance(residue_sasa_sum, dict):\n",
    "        # This one is already position-based (1-based indexing)\n",
    "        for pos, value in residue_sasa_sum.items():\n",
    "            # Make sure position exists in dataframe\n",
    "            if (df['position'] == pos).any():\n",
    "                df.loc[df['position'] == pos, 'residue_sasa_sum'] = value\n",
    "            else:\n",
    "                print(f\"Warning: Position {pos} in SASA data not found in dataframe\")\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    base_cols = ['position', 'amino_acid', 'region']\n",
    "    structural_cols = ['disulfide_bond', 'sap', 'high_sap', 'sidechain_sasa', \n",
    "                      'buried', 'pka', 'pka_shift', 'residue_sasa_sum']\n",
    "    hydro_cols = [col for col in df.columns if col.startswith('hydrophobicity') or col == 'charge_sign']\n",
    "    \n",
    "    # Get any remaining columns\n",
    "    other_cols = [col for col in df.columns if col not in base_cols + structural_cols + hydro_cols]\n",
    "    \n",
    "    # New column order\n",
    "    new_order = base_cols + structural_cols + hydro_cols + other_cols\n",
    "    available_cols = [col for col in new_order if col in df.columns]\n",
    "    df = df[available_cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test antibody sequences (based on therapeutic antibodies)\n",
    "HEAVY_SEQUENCE = (\n",
    "    \"QVQLVQSGAEVKKPGASVKVSCKASGGTFSSYAISWVRQAPGQGLEWMGGIIPIFGTANYAQKFQGRVTITADESTSTAYMELSSLRSEDTAVYYCARSHYGLDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSCDKTHTCPPCPAPELLGGPSVFLFPPKPKDTLMISRTPEVTCVVVDVSHEDPEVKFNWYVDGVEVHNAKTKPREEQYASTYRVVSVLTVLHQDWLNGKEYKCKVSNKALPAPIEKTISKAKGQPREPQVYTLPPSRDELTKNQVSLTCLVKGFYPSDIAVEWESNGQPENNYKTTPPVLDSDGSFFLYSKLTVDKSRWQQGNVFSCSVMHEALHNHYTQKSLSLSPGK\"\n",
    ")\n",
    "# Light chain: Includes realistic VL domain + human kappa constant region  \n",
    "LIGHT_SEQUENCE = (\n",
    "    \"DIQMTQSPSSLSASVGDRVTITCRASHSISSYLAWYQQKPGKAPKLLIYAASSLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSTPLTFGGGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC\"\n",
    ")\n",
    "\n",
    "# Extract the liabilities list from the DataFrame\n",
    "# Assuming 'liabilities' is a DataFrame with a column called 'liabilities' containing the list\n",
    "liabilities_list = liabilities['liabilities'].iloc[0]  # Get the list from the first row\n",
    "structures_results_seq = add_residue_sasa_sum_column(structure_results_seq)\n",
    "structures_data = structures_results_seq.iloc[0]\n",
    "\n",
    "\n",
    "# Create comprehensive dataframes for both chains\n",
    "df_heavy = create_comprehensive_df(annotated_H, heavy_profiles, 'Heavy')\n",
    "df_light = create_comprehensive_df(annotated_L, light_profiles, 'Light')\n",
    "\n",
    "# Add liability columns to both dataframes\n",
    "df_heavy = add_liability_columns(df_heavy, 'Heavy', liabilities_list)\n",
    "df_light = add_liability_columns(df_light, 'Light', liabilities_list)\n",
    "\n",
    "\n",
    "# Add structural information to both dataframes\n",
    "df_heavy = add_structural_info_to_df(df_heavy, structures_data, 'Heavy')\n",
    "df_light = add_structural_info_to_df(df_light, structures_data, 'Light')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structures_results_seq.drop(\"residue_sasa\", axis =1 ).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_results_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the function to your dataframe\n",
    "structures_results_seq = add_residue_sasa_sum_column(structure_results_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures_results_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now use the df to add info /cols to the one with out of liability, so precise disulfide bond cysteine, but also the sap per position, the one with high sap with True and others false, the side chain sasa per position, the buried residues as true others as false, the pka per residues , if no pka then None, same for pka shift and the charge, and the residue_sasa_sum per position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disulfide_bonds\tresidue_sap\thigh_sap_residues\tresidue_sasa\tsidechain_sasa\trelative_sasa\tburied_residues\tresidue_pka\tpka_shifts\tcharge_profile\tstability_profile\tresidue_sasa_sum\n",
    "0\t[{'cys1': 'H_21', 'cys2': 'H_95', 'distance': ...\t{'ASP_0_H': 0.029531839957845273, 'VAL_1_H': 0...\t[LEU_10_H, PRO_40_H, TYR_58_H, LEU_101_H, TYR_...\t{'ASP_0_H': {'N': 40.450005754993995, 'CA': 9....\t{'ASP_0_H': 111.79910841184281, 'VAL_1_H': 31....\t{}\t[LEU_3_H, GLU_5_H, LEU_19_H, CYS_21_H, VAL_28_...\t0 {'ASP_54_H': 2.55, 'ASP_61_H': 2.76, 'ASP...\t0 {'ASP_54_H': -1.25, 'ASP_61_H': -1.04, 'A...\t0 {0.0: {'folded': 11.99, 'unfolded': 12.0}...\t0 {0.0: 11.82, 1.0: 11.76, 2.0: 11.18, 3.0:...\t{1: 111.79910841184281, 2: 31.78042001323901, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chain annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peptide_results\n",
    "heavy_valid, light_valid\n",
    "cdrs_H, cdrs_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Antibody annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_results\n",
    "peptide_results\n",
    "heavy_valid, light_valid\n",
    "cdrs_H, cdrs_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "abxtract",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python (abxtract)",
   "language": "python",
   "name": "abxtract"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
